{"cells":[{"cell_type":"markdown","id":"unknown_id_1","metadata":{"id":"unknown_id_1"},"source":["# Logistic Regression: Gradient Descent with Log Loss\n","\n","\n","Logistic Regression is used when the output variable $y$ is **binary** (0 or 1).\n","The goal is to learn parameters $w$ and $b$ such that the predicted probability:\n","\n","$$\n","\\hat{y} = P(y=1 \\mid x)\n","$$\n","\n","matches the true labels as closely as possible.\n","\n","---\n","\n","# 1. The Hypothesis Function\n","\n","Logistic Regression uses the **sigmoid function** on a linear model:\n","\n","$$\n","z = wx + b\n","$$\n","\n","$$\n","\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n","$$\n","\n","Interpretation:\n","\n","- If $\\hat{y} \\approx 1$, the model predicts class **1**\n","- If $\\hat{y} \\approx 0$, the model predicts class **0**\n","\n","---\n","\n","# 2. Loss Function: Log Loss\n","(also called Binary Cross-Entropy)\n","\n","For each training example:\n","\n","$$\n","L_i = -\\left[\n","y_i \\log(\\hat{y}_i)\n","+\n","(1 - y_i)\\log(1 - \\hat{y}_i)\n","\\right]\n","$$\n","\n","Properties:\n","\n","- If $y_i = 1$, only the first term remains:\n","  $$\n","  L_i = -\\log(\\hat{y}_i)\n","  $$\n","- If $y_i = 0$, only the second term remains:\n","  $$\n","  L_i = -\\log(1 - \\hat{y}_i)\n","  $$\n","\n","The total cost over all examples:\n","\n","$$\n","J(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} L_i\n","$$\n","\n","Our goal:\n","$$\n","\\min_{w,b} J(w,b)\n","$$\n","\n","---\n","\n","# 3. The Chain Rule Structure\n","\n","We must compute:\n","\n","$$\n","\\frac{\\partial J}{\\partial w},\n","\\qquad\n","\\frac{\\partial J}{\\partial b}\n","$$\n","\n","For one example, the computation flows like this:\n","\n","$$\n","w, b\n","\\quad \\longrightarrow \\quad\n","z = wx + b\n","\\quad \\longrightarrow \\quad\n","\\hat{y} = \\sigma(z)\n","\\quad \\longrightarrow \\quad\n","L\n","$$\n","\n","Applying the chain rule:\n","\n","$$\n","\\frac{\\partial L}{\\partial w}\n","=\n","\\frac{\\partial L}{\\partial \\hat{y}}\n","\\cdot\n","\\frac{\\partial \\hat{y}}{\\partial z}\n","\\cdot\n","\\frac{\\partial z}{\\partial w}\n","$$\n","\n","$$\n","\\frac{\\partial L}{\\partial b}\n","=\n","\\frac{\\partial L}{\\partial \\hat{y}}\n","\\cdot\n","\\frac{\\partial \\hat{y}}{\\partial z}\n","\\cdot\n","\\frac{\\partial z}{\\partial b}\n","$$\n","\n","We compute each component next.\n","\n","---\n","\n","# 4. Step-by-Step Derivatives\n","\n","## 4.1 Derivative of the Sigmoid Function\n","\n","$$\n","\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n","$$\n","\n","A very useful identity:\n","\n","$$\n","\\frac{d\\hat{y}}{dz} = \\hat{y}(1 - \\hat{y})\n","$$\n","\n","---\n","\n","## 4.2 Derivative of Log Loss w.r.t. Prediction $\\hat{y}$\n","\n","$$\n","L = -\\left[\n","y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y})\n","\\right]\n","$$\n","\n","Differentiate term-by-term:\n","\n","$$\n","\\frac{\\partial L}{\\partial \\hat{y}}\n","=\n","-\\frac{y}{\\hat{y}}\n","+\n","\\frac{1 - y}{1 - \\hat{y}}\n","$$\n","\n","---\n","\n","## 4.3 Derivative of $z$\n","\n","$$\n","z = wx + b\n","$$\n","\n","$$\n","\\frac{\\partial z}{\\partial w} = x,\n","\\qquad\n","\\frac{\\partial z}{\\partial b} = 1\n","$$\n","\n","---\n","\n","# 5. Combine Using Chain Rule\n","\n","### First compute $\\frac{\\partial L}{\\partial z}$:\n","\n","$$\n","\\frac{\\partial L}{\\partial z}\n","=\n","\\frac{\\partial L}{\\partial \\hat{y}}\n","\\cdot\n","\\frac{\\partial \\hat{y}}{\\partial z}\n","$$\n","\n","Substitute:\n","\n","$$\n","\\frac{\\partial L}{\\partial z}\n","=\n","\\left(\n","-\\frac{y}{\\hat{y}}\n","+\n","\\frac{1-y}{1-\\hat{y}}\n","\\right)\n","\\cdot\n","\\hat{y}(1 - \\hat{y})\n","$$\n","\n","Simplify each term:\n","\n","$$\n","-\\frac{y}{\\hat{y}} \\cdot \\hat{y}(1 - \\hat{y})\n","= -y(1 - \\hat{y})\n","$$\n","\n","$$\n","\\frac{1-y}{1-\\hat{y}} \\cdot \\hat{y}(1 - \\hat{y})\n","= (1 - y)\\hat{y}\n","$$\n","\n","Combine:\n","\n","$$\n","\\frac{\\partial L}{\\partial z}\n","= -y(1-\\hat{y}) + (1-y)\\hat{y}\n","$$\n","\n","Expand:\n","\n","$$\n","= -y + y\\hat{y} + \\hat{y} - y\\hat{y}\n","$$\n","\n","$$\n","= \\hat{y} - y\n","$$\n","\n","---\n","\n","# 6. Final Gradients for One Example\n","\n","Using:\n","\n","$$\n","\\frac{\\partial z}{\\partial w} = x,\n","\\qquad\n","\\frac{\\partial z}{\\partial b} = 1\n","$$\n","\n","### Gradient w.r.t. w:\n","$$\n","\\frac{\\partial L}{\\partial w}\n","=\n","(\\hat{y} - y)x\n","$$\n","\n","### Gradient w.r.t. b:\n","$$\n","\\frac{\\partial L}{\\partial b}\n","=\n","\\hat{y} - y\n","$$\n","\n","---\n","\n","# 7. Gradients for All Training Samples\n","\n","Since:\n","\n","$$\n","J(w,b) = \\frac{1}{n} \\sum L_i\n","$$\n","\n","Then:\n","\n","$$\n","\\frac{\\partial J}{\\partial w}\n","=\n","\\frac{1}{n}\n","\\sum_{i=1}^{n}\n","(\\hat{y}_i - y_i)x_i\n","$$\n","\n","$$\n","\\frac{\\partial J}{\\partial b}\n","=\n","\\frac{1}{n}\n","\\sum_{i=1}^{n}\n","(\\hat{y}_i - y_i)\n","$$\n","\n","---\n","\n","# 8. Gradient Descent Update Rules\n","\n","Given learning rate $\\alpha$:\n","\n","$$\n","w \\leftarrow w - \\alpha \\frac{\\partial J}{\\partial w}\n","$$\n","\n","$$\n","b \\leftarrow b - \\alpha \\frac{\\partial J}{\\partial b}\n","$$\n","\n","Substitute the gradients:\n","\n","$$\n","w \\leftarrow\n","w - \\alpha \\left(\n","\\frac{1}{n}\n","\\sum_{i=1}^{n}\n","(\\hat{y}_i - y_i)x_i\n","\\right)\n","$$\n","\n","$$\n","b \\leftarrow\n","b - \\alpha \\left(\n","\\frac{1}{n}\n","\\sum_{i=1}^{n}\n","(\\hat{y}_i - y_i)\n","\\right)\n","$$\n","\n","---"]},{"cell_type":"code","source":["import numpy as np\n","\n","# -----------------------------\n","# 1. Sample Data (6 values)\n","# -----------------------------\n","X = np.array([1, 2, 3, 4, 5, 6], dtype=float) # number of hours studied\n","y = np.array([0, 0, 0, 1, 1, 1], dtype=float) # Pass=1 and fail =0\n","\n","n = len(X)\n","\n","# -----------------------------\n","# 2. Sigmoid function\n","# -----------------------------\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","# -----------------------------\n","# 3. Log Loss Cost Function\n","# -----------------------------\n","def compute_cost(w, b, X, y):\n","    z = w * X + b\n","    y_hat = sigmoid(z)\n","    cost = -np.mean(y * np.log(y_hat + 1e-9) + (1 - y)*np.log(1 - y_hat + 1e-9))\n","    return cost\n","\n","# -----------------------------\n","# 4. Compute gradients dw, db\n","# -----------------------------\n","def compute_gradients(w, b, X, y):\n","    z = w * X + b\n","    y_hat = sigmoid(z)\n","    error = y_hat - y\n","    dw = np.mean(error * X)\n","    db = np.mean(error)\n","    return dw, db\n","\n","# -----------------------------\n","# 5. Gradient Descent\n","# -----------------------------\n","w = 0.0\n","b = 0.0\n","alpha = 0.1\n","iterations = 2000\n","\n","cost_history = []\n","\n","for i in range(iterations):\n","    dw, db = compute_gradients(w, b, X, y)\n","\n","    # Update parameters\n","    w -= alpha * dw\n","    b -= alpha * db\n","\n","    # Save cost for monitoring\n","    cost = compute_cost(w, b, X, y)\n","    cost_history.append(cost)\n","\n","    # Print every 200 steps\n","    if i % 200 == 0:\n","        print(f\"Iteration {i}: Cost = {cost:.6f}, w = {w:.4f}, b = {b:.4f}\")\n","\n","# -----------------------------\n","# 6. Final parameters\n","# -----------------------------\n","print(\"\\nTraining complete!\")\n","print(f\"Final weight: w = {w:.4f}\")\n","print(f\"Final bias  : b = {b:.4f}\")\n","\n","# -----------------------------\n","# 7. Try predictions\n","# -----------------------------\n","def predict(x):\n","    prob = sigmoid(w * x + b)\n","    return 1 if prob >= 0.5 else 0\n","\n","test_values = [1, 2, 3, 4, 5, 6]\n","\n","print(\"\\nPredictions:\")\n","for x in test_values:\n","    print(f\"Hours {x} → Predicted: {predict(x)} (Prob = {sigmoid(w*x+b):.4f})\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-cOrY4jcWuE","executionInfo":{"status":"ok","timestamp":1764333158752,"user_tz":-330,"elapsed":172,"user":{"displayName":"Yuvaraju Maddiboina","userId":"13345105802969388848"}},"outputId":"84bbf089-59fa-44be-f1db-05aacad201d3"},"id":"S-cOrY4jcWuE","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0: Cost = 0.647499, w = 0.0750, b = 0.0000\n","Iteration 200: Cost = 0.324548, w = 0.7819, b = -2.2807\n","Iteration 400: Cost = 0.236686, w = 1.1202, b = -3.5519\n","Iteration 600: Cost = 0.194605, w = 1.3592, b = -4.4346\n","Iteration 800: Cost = 0.169109, w = 1.5473, b = -5.1223\n","Iteration 1000: Cost = 0.151564, w = 1.7043, b = -5.6929\n","Iteration 1200: Cost = 0.138511, w = 1.8405, b = -6.1851\n","Iteration 1400: Cost = 0.128278, w = 1.9615, b = -6.6208\n","Iteration 1600: Cost = 0.119952, w = 2.0709, b = -7.0139\n","Iteration 1800: Cost = 0.112987, w = 2.1712, b = -7.3733\n","\n","Training complete!\n","Final weight: w = 2.2637\n","Final bias  : b = -7.7039\n","\n","Predictions:\n","Hours 1 → Predicted: 0 (Prob = 0.0043)\n","Hours 2 → Predicted: 0 (Prob = 0.0401)\n","Hours 3 → Predicted: 0 (Prob = 0.2864)\n","Hours 4 → Predicted: 1 (Prob = 0.7943)\n","Hours 5 → Predicted: 1 (Prob = 0.9738)\n","Hours 6 → Predicted: 1 (Prob = 0.9972)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bxD1BCW4c1-K"},"id":"bxD1BCW4c1-K","execution_count":null,"outputs":[]}],"metadata":{"language":"markdown","colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}