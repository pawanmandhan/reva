{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84663edd",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d55d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A.1: Data Loading (1 Mark)\n",
    "\n",
    "**Objective**: Load the Wisconsin Breast Cancer dataset and examine its structure.\n",
    "\n",
    "### Dataset Information:\n",
    "- **Source**: Wisconsin Breast Cancer Database\n",
    "- **Features**: 30 numerical features derived from biopsy images\n",
    "- **Target**: Binary classification (Malignant=1, Benign=0)\n",
    "- **Samples**: ~569 patient records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e385e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_path = 'Wisconsin.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(\"Dataset Loaded Successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn Names ({len(df.columns)} features):\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9702dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f468e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c784e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A.2: Data Cleaning (2 Marks)\n",
    "\n",
    "**Objective**: Handle missing values, drop unnecessary columns (like ID), and prepare clean data.\n",
    "\n",
    "### Steps:\n",
    "1. Check for missing values\n",
    "2. Identify and drop ID columns (if any)\n",
    "3. Separate features (X) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56552261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Report:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (if any)\n",
    "# Since this dataset typically has no missing values, we'll demonstrate the approach\n",
    "print(\"Dataset before cleaning:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Remove any rows with missing values (best practice for medical data)\n",
    "df_cleaned = df.dropna()\n",
    "print(f\"\\nDataset after removing missing values:\")\n",
    "print(f\"Shape: {df_cleaned.shape}\")\n",
    "print(f\"Rows removed: {df.shape[0] - df_cleaned.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from features\n",
    "# The 'target' column contains the diagnosis (1=Malignant, 0=Benign)\n",
    "y = df_cleaned['target'].values  # Convert to numpy array\n",
    "X = df_cleaned.drop('target', axis=1).values  # Drop target, convert to numpy array\n",
    "\n",
    "feature_names = df_cleaned.drop('target', axis=1).columns.tolist()\n",
    "\n",
    "print(f\"Features shape (X): {X.shape}\")\n",
    "print(f\"Target shape (y): {y.shape}\")\n",
    "print(f\"\\nFeature names ({len(feature_names)} features):\")\n",
    "for i, name in enumerate(feature_names, 1):\n",
    "    print(f\"  {i}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distribution\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "print(\"Class Distribution:\")\n",
    "print(f\"Benign (0): {class_counts[0]} samples ({class_counts[0]/len(y)*100:.2f}%)\")\n",
    "print(f\"Malignant (1): {class_counts[1]} samples ({class_counts[1]/len(y)*100:.2f}%)\")\n",
    "print(f\"Total: {len(y)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3d59c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A.3: Feature Scaling (2 Marks)\n",
    "\n",
    "**Objective**: Implement feature scaling methods to normalize data for better Gradient Descent convergence.\n",
    "\n",
    "### Why Scaling?\n",
    "- Features have different ranges (e.g., area ranges 0-2500, smoothness ranges 0-0.3)\n",
    "- Gradient Descent converges faster with normalized features\n",
    "- Prevents features with larger scales from dominating\n",
    "\n",
    "### Methods:\n",
    "1. **Min-Max Normalization**: Scales values to [0, 1] range\n",
    "2. **Standardization (Z-score)**: Centers data with mean=0, std=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c046b62",
   "metadata": {},
   "source": [
    "### 3.1: Min-Max Normalization\n",
    "\n",
    "**Formula**:\n",
    "$$X_{normalized} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "This scales all features to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(X):\n",
    "    \"\"\"\n",
    "    Apply Min-Max normalization to features.\n",
    "    \n",
    "    Parameters:\n",
    "    X (np.ndarray): Feature matrix of shape (m, n) where m=samples, n=features\n",
    "    \n",
    "    Returns:\n",
    "    X_normalized (np.ndarray): Normalized feature matrix\n",
    "    min_vals (np.ndarray): Minimum values for each feature (for inverse transform)\n",
    "    max_vals (np.ndarray): Maximum values for each feature (for inverse transform)\n",
    "    \"\"\"\n",
    "    # Calculate min and max for each feature (column-wise)\n",
    "    min_vals = np.min(X, axis=0)\n",
    "    max_vals = np.max(X, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    ranges = max_vals - min_vals\n",
    "    ranges[ranges == 0] = 1  # If feature has no variation, set range to 1\n",
    "    \n",
    "    # Apply normalization\n",
    "    X_normalized = (X - min_vals) / ranges\n",
    "    \n",
    "    return X_normalized, min_vals, max_vals\n",
    "\n",
    "# Apply Min-Max normalization\n",
    "X_minmax, X_min, X_max = min_max_normalize(X)\n",
    "\n",
    "print(\"Min-Max Normalization Applied!\")\n",
    "print(f\"\\nNormalized Data Range: [{X_minmax.min():.4f}, {X_minmax.max():.4f}]\")\n",
    "print(f\"Original Data Range: [{X.min():.4f}, {X.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify normalization on first 3 features\n",
    "print(\"Verification - First 3 features (first 5 samples):\\n\")\n",
    "print(\"Original Data:\")\n",
    "print(X[:5, :3])\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(X_minmax[:5, :3])\n",
    "print(\"\\nFeature-wise Min and Max after normalization:\")\n",
    "for i in range(3):\n",
    "    print(f\"Feature {i+1}: Min={X_minmax[:, i].min():.6f}, Max={X_minmax[:, i].max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f3b17",
   "metadata": {},
   "source": [
    "### 3.2: Standardization (Z-score Normalization)\n",
    "\n",
    "**Formula**:\n",
    "$$X_{standardized} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean of the feature\n",
    "- $\\sigma$ = standard deviation of the feature\n",
    "\n",
    "This centers data around 0 with unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab070c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    Apply Standardization (Z-score normalization) to features.\n",
    "    \n",
    "    Parameters:\n",
    "    X (np.ndarray): Feature matrix of shape (m, n)\n",
    "    \n",
    "    Returns:\n",
    "    X_standardized (np.ndarray): Standardized feature matrix\n",
    "    mean_vals (np.ndarray): Mean values for each feature\n",
    "    std_vals (np.ndarray): Standard deviation values for each feature\n",
    "    \"\"\"\n",
    "    # Calculate mean and standard deviation for each feature\n",
    "    mean_vals = np.mean(X, axis=0)\n",
    "    std_vals = np.std(X, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    std_vals[std_vals == 0] = 1\n",
    "    \n",
    "    # Apply standardization\n",
    "    X_standardized = (X - mean_vals) / std_vals\n",
    "    \n",
    "    return X_standardized, mean_vals, std_vals\n",
    "\n",
    "# Apply Standardization\n",
    "X_standard, X_mean, X_std = standardize(X)\n",
    "\n",
    "print(\"Standardization Applied!\")\n",
    "print(f\"\\nStandardized Data Mean: {X_standard.mean():.6f}\")\n",
    "print(f\"Standardized Data Std Dev: {X_standard.std():.6f}\")\n",
    "print(f\"\\nOriginal Data Mean: {X.mean():.6f}\")\n",
    "print(f\"Original Data Std Dev: {X.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify standardization on first 3 features\n",
    "print(\"Verification - First 3 features (first 5 samples):\\n\")\n",
    "print(\"Original Data:\")\n",
    "print(X[:5, :3])\n",
    "print(\"\\nStandardized Data:\")\n",
    "print(X_standard[:5, :3])\n",
    "print(\"\\nFeature-wise Mean and Std after standardization:\")\n",
    "for i in range(3):\n",
    "    print(f\"Feature {i+1}: Mean={X_standard[:, i].mean():.6f}, Std={X_standard[:, i].std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce523c7d",
   "metadata": {},
   "source": [
    "### 3.3: Comparison of Scaling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c780d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two scaling methods visually\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original Data\n",
    "axes[0].hist(X[:, 0], bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Original Data\\n(Feature 1: Mean Radius)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Min-Max Normalized\n",
    "axes[1].hist(X_minmax[:, 0], bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Min-Max Normalized\\n(Range: [0, 1])', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Standardized\n",
    "axes[2].hist(X_standard[:, 0], bins=30, color='red', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_title('Standardized\\n(Mean≈0, Std≈1)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Value')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scaling_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Scaling methods comparison plotted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b433a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': [\n",
    "        X[:, 0].min(), X[:, 0].max(), X[:, 0].mean(), X[:, 0].std()\n",
    "    ],\n",
    "    'Min-Max Normalized': [\n",
    "        X_minmax[:, 0].min(), X_minmax[:, 0].max(), X_minmax[:, 0].mean(), X_minmax[:, 0].std()\n",
    "    ],\n",
    "    'Standardized': [\n",
    "        X_standard[:, 0].min(), X_standard[:, 0].max(), X_standard[:, 0].mean(), X_standard[:, 0].std()\n",
    "    ]\n",
    "}, index=['Min', 'Max', 'Mean', 'Std Dev'])\n",
    "\n",
    "print(\"\\nStatistics Comparison (Feature 1: Mean Radius):\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46439d15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A.4: Exploratory Data Analysis (EDA) - Bonus\n",
    "\n",
    "**Objective**: Visualize feature distributions, correlations, and class balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40edb8db",
   "metadata": {},
   "source": [
    "### 4.1: Target Distribution (Class Balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73dce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Count plot\n",
    "classes = ['Benign (0)', 'Malignant (1)']\n",
    "counts = np.bincount(y)\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "axes[0].bar(classes, counts, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0].set_title('Class Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, count in enumerate(counts):\n",
    "    axes[0].text(i, count + 5, str(count), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(counts, labels=classes, colors=colors, autopct='%1.1f%%', startangle=90,\n",
    "            textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Class Balance Percentage', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Class distribution visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8e5a1",
   "metadata": {},
   "source": [
    "### 4.2: Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of first 12 features\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(12):\n",
    "    axes[i].hist(X[:, i], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'{feature_names[i]}', fontsize=10, fontweight='bold')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature distributions plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124731a3",
   "metadata": {},
   "source": [
    "### 4.3: Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a008b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix with target variable\n",
    "df_with_target = np.column_stack([X, y])\n",
    "column_names = feature_names + ['target']\n",
    "df_corr = pd.DataFrame(df_with_target, columns=column_names)\n",
    "\n",
    "# Calculate correlation with target\n",
    "correlations_with_target = df_corr.corr()['target'].drop('target').sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 Features Most Correlated with Target (Malignancy):\")\n",
    "print(correlations_with_target.head(10))\n",
    "print(\"\\nBottom 10 Features Least Correlated with Target:\")\n",
    "print(correlations_with_target.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation with target\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = ['red' if x < 0 else 'green' for x in correlations_with_target]\n",
    "ax.barh(range(len(correlations_with_target)), correlations_with_target.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_yticks(range(len(correlations_with_target)))\n",
    "ax.set_yticklabels(correlations_with_target.index, fontsize=9)\n",
    "ax.set_xlabel('Correlation Coefficient', fontweight='bold')\n",
    "ax.set_title('Feature Correlations with Target (Malignancy)', fontsize=12, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_correlations.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature correlations plotted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of correlations between first 15 features\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "corr_matrix = df_corr.iloc[:, :15].corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "ax.set_title('Correlation Matrix - First 15 Features', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation heatmap plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d9f70",
   "metadata": {},
   "source": [
    "### 4.4: Distribution by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857683e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for top 6 features by correlation\n",
    "top_features_indices = correlations_with_target.abs().nlargest(6).index\n",
    "top_features_names = [feature_names[feature_names.index(f)] if f in feature_names else f \n",
    "                      for f in top_features_indices]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features_indices[:6]):\n",
    "    feature_idx = feature_names.index(feature)\n",
    "    \n",
    "    benign_data = X[y == 0, feature_idx]\n",
    "    malignant_data = X[y == 1, feature_idx]\n",
    "    \n",
    "    axes[idx].boxplot([benign_data, malignant_data], labels=['Benign', 'Malignant'],\n",
    "                       patch_artist=True, boxprops=dict(facecolor='lightblue'),\n",
    "                       medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_title(feature, fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distributions.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Class-wise distributions plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda7ec93",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Data Preparation Complete\n",
    "\n",
    "We have successfully completed **Part A: Data Exploration & Preprocessing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"PART A SUMMARY: DATA EXPLORATION & PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✓ DATA LOADING:\")\n",
    "print(f\"  - Dataset shape: {df.shape}\")\n",
    "print(f\"  - Features: {X.shape[1]}\")\n",
    "print(f\"  - Samples: {X.shape[0]}\")\n",
    "\n",
    "print(\"\\n✓ DATA CLEANING:\")\n",
    "print(f\"  - Missing values: {df_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"  - ID columns dropped: Yes\")\n",
    "print(f\"  - Final clean samples: {len(y)}\")\n",
    "\n",
    "print(\"\\n✓ TARGET DISTRIBUTION:\")\n",
    "print(f\"  - Benign (0): {(y==0).sum()} samples ({(y==0).sum()/len(y)*100:.1f}%)\")\n",
    "print(f\"  - Malignant (1): {(y==1).sum()} samples ({(y==1).sum()/len(y)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n✓ FEATURE SCALING IMPLEMENTED:\")\n",
    "print(f\"  - Min-Max Normalization: ✓\")\n",
    "print(f\"    * Range: [0, 1]\")\n",
    "print(f\"    * Status: X_minmax array created (shape: {X_minmax.shape})\")\n",
    "print(f\"\\n  - Standardization (Z-score): ✓\")\n",
    "print(f\"    * Mean ≈ 0, Std ≈ 1\")\n",
    "print(f\"    * Status: X_standard array created (shape: {X_standard.shape})\")\n",
    "\n",
    "print(\"\\n✓ EXPLORATORY DATA ANALYSIS (BONUS):\")\n",
    "print(f\"  - Class balance visualization: ✓\")\n",
    "print(f\"  - Feature distributions: ✓\")\n",
    "print(f\"  - Feature correlations: ✓\")\n",
    "print(f\"  - Class-wise comparisons: ✓\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY FOR PART B: THE MATHEMATICS OF LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5580892",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights from Data Preprocessing\n",
    "\n",
    "### 1. Dataset Quality\n",
    "- The Wisconsin Breast Cancer dataset is clean with no missing values\n",
    "- Well-balanced classes (approximately 63% Benign, 37% Malignant)\n",
    "\n",
    "### 2. Feature Characteristics\n",
    "- Features have widely varying scales (e.g., area: 0-2500 vs. smoothness: 0-0.3)\n",
    "- This variation necessitates scaling before applying Gradient Descent\n",
    "\n",
    "### 3. Feature-Target Relationships\n",
    "- Strong correlations exist between specific tumor measurements and malignancy\n",
    "- Features like \"worst radius\" and \"worst area\" show high correlation with target\n",
    "- These features will be important in the logistic regression model\n",
    "\n",
    "### 4. Scaling Method Recommendation\n",
    "- **For this project**: We recommend **Standardization (Z-score)** because:\n",
    "  - Works better with Gradient Descent algorithm\n",
    "  - Provides better convergence properties\n",
    "  - More interpretable in healthcare context (deviations from mean)\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "We will use the standardized features `X_standard` for training the logistic regression model in **Part B**, where we'll implement:\n",
    "1. Sigmoid function\n",
    "2. Hypothesis function\n",
    "3. Cost function (Binary Cross Entropy)\n",
    "4. Gradient Descent algorithm\n",
    "5. Learning curves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
