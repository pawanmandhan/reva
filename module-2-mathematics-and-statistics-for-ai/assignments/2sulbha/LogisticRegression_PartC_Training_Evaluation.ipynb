{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2615af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"All imports done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd4ec61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: X=(569, 30), y=(569,)\n"
     ]
    }
   ],
   "source": [
    "# Load data from Part A\n",
    "df = pd.read_csv('Wisconsin.csv')\n",
    "y = df['target'].values\n",
    "X = df.drop('target', axis=1).values\n",
    "feature_names = df.drop('target', axis=1).columns.tolist()\n",
    "\n",
    "print(f\"Data loaded. Shape: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30243d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features standardized\n"
     ]
    }
   ],
   "source": [
    "# Standardize features - same as Part A\n",
    "def standardize(X):\n",
    "    mean_vals = np.mean(X, axis=0)\n",
    "    std_vals = np.std(X, axis=0)\n",
    "    std_vals[std_vals == 0] = 1\n",
    "    X_std = (X - mean_vals) / std_vals\n",
    "    return X_std, mean_vals, std_vals\n",
    "\n",
    "X_scaled, mean, std = standardize(X)\n",
    "print(\"Features standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5761b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions loaded from Part B\n"
     ]
    }
   ],
   "source": [
    "# Functions from Part B\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def hypothesis(X, w, b):\n",
    "    z = np.dot(X, w) + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "def compute_cost(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    h = hypothesis(X, w, b)\n",
    "    epsilon = 1e-15\n",
    "    h = np.clip(h, epsilon, 1 - epsilon)\n",
    "    cost = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    h = hypothesis(X, w, b)\n",
    "    error = h - y\n",
    "    dw = np.dot(X.T, error) / m\n",
    "    db = np.mean(error)\n",
    "    return dw, db\n",
    "\n",
    "print(\"Functions loaded from Part B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120fdcce",
   "metadata": {},
   "source": [
    "---\n",
    "# Part C1: Manual Data Partitioning (2 Marks)\n",
    "\n",
    "Implement a train/test split from scratch without external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f0b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split function created\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split data into train and test sets manually using numpy\n",
    "    \n",
    "    Args:\n",
    "        X: features\n",
    "        y: target\n",
    "        test_size: fraction for test set (default 0.2 = 80/20 split)\n",
    "        random_state: seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Set seed if provided\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Create shuffled indices\n",
    "    indices = np.arange(m)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate split point\n",
    "    split_idx = int((1 - test_size) * m)\n",
    "    \n",
    "    # Split indices into train and test\n",
    "    train_indices = indices[:split_idx]\n",
    "    test_indices = indices[split_idx:]\n",
    "    \n",
    "    # Use indices to split X and y\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print(\"Train-test split function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31f7f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 455 samples\n",
      "Test set: 114 samples\n",
      "Total samples: 569\n",
      "\n",
      "Train/Test ratio: 80.0% / 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Apply the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Total samples: {X_train.shape[0] + X_test.shape[0]}\")\n",
    "print(f\"\\nTrain/Test ratio: {X_train.shape[0] / (X_train.shape[0] + X_test.shape[0]) * 100:.1f}% / {X_test.shape[0] / (X_train.shape[0] + X_test.shape[0]) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b866abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution:\n",
      "  Benign (0): 290 (63.7%)\n",
      "  Malignant (1): 165 (36.3%)\n",
      "\n",
      "Test set class distribution:\n",
      "  Benign (0): 67 (58.8%)\n",
      "  Malignant (1): 47 (41.2%)\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution in train and test\n",
    "train_class_0 = (y_train == 0).sum()\n",
    "train_class_1 = (y_train == 1).sum()\n",
    "test_class_0 = (y_test == 0).sum()\n",
    "test_class_1 = (y_test == 1).sum()\n",
    "\n",
    "print(\"Training set class distribution:\")\n",
    "print(f\"  Benign (0): {train_class_0} ({train_class_0/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Malignant (1): {train_class_1} ({train_class_1/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTest set class distribution:\")\n",
    "print(f\"  Benign (0): {test_class_0} ({test_class_0/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Malignant (1): {test_class_1} ({test_class_1/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598223ef",
   "metadata": {},
   "source": [
    "---\n",
    "# Part C2: Fitting the Model (3 Marks)\n",
    "\n",
    "Execute Gradient Descent using the training partition to estimate optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba276210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent function ready\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, iterations=1000, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Train logistic regression model using gradient descent\n",
    "    \n",
    "    Args:\n",
    "        X: training features\n",
    "        y: training labels\n",
    "        iterations: number of iterations\n",
    "        learning_rate: step size\n",
    "    \n",
    "    Returns:\n",
    "        w: learned weights\n",
    "        b: learned bias\n",
    "        costs: cost history for plotting\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initialize parameters\n",
    "    w = np.zeros(n)\n",
    "    b = 0\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for i in range(iterations):\n",
    "        # Compute gradients\n",
    "        dw, db = compute_gradients(X, y, w, b)\n",
    "        \n",
    "        # Update weights and bias\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Store cost\n",
    "        cost = compute_cost(X, y, w, b)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"Iteration {i+1}: Cost = {cost:.6f}\")\n",
    "    \n",
    "    return w, b, costs\n",
    "\n",
    "print(\"Gradient descent function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on training set\n",
    "print(\"Starting training on training set...\\n\")\n",
    "w_final, b_final, cost_history = gradient_descent(X_train, y_train, iterations=3000, learning_rate=0.1)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Initial cost: {cost_history[0]:.6f}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.6f}\")\n",
    "print(f\"Cost reduction: {cost_history[0] - cost_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve on training set\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_history, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Training Cost vs Iterations')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cost_history[-500:], 'r-', linewidth=2)\n",
    "plt.xlabel('Iteration (Last 500)')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Training Cost - Last 500 Iterations')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf13b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model on training data\n",
    "train_predictions_prob = hypothesis(X_train, w_final, b_final)\n",
    "print(f\"Training set predictions statistics:\")\n",
    "print(f\"  Min probability: {train_predictions_prob.min():.6f}\")\n",
    "print(f\"  Max probability: {train_predictions_prob.max():.6f}\")\n",
    "print(f\"  Mean probability: {train_predictions_prob.mean():.6f}\")\n",
    "\n",
    "# Check training cost\n",
    "train_cost = compute_cost(X_train, y_train, w_final, b_final)\n",
    "print(f\"\\nTraining cost: {train_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5e3a0",
   "metadata": {},
   "source": [
    "---\n",
    "# Part C3: Generating Predictions (1 Mark)\n",
    "\n",
    "Use the optimized parameters to classify unseen test observations using a decision threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d208fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, w, b, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make binary predictions from probabilities\n",
    "    \n",
    "    Args:\n",
    "        X: features\n",
    "        w: weights\n",
    "        b: bias\n",
    "        threshold: decision threshold (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: binary class predictions (0 or 1)\n",
    "        probabilities: predicted probabilities\n",
    "    \"\"\"\n",
    "    probabilities = hypothesis(X, w, b)\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "    return predictions, probabilities\n",
    "\n",
    "print(\"Prediction function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred_test, y_pred_prob_test = make_predictions(X_test, w_final, b_final, threshold=0.5)\n",
    "\n",
    "print(f\"Test set predictions made\")\n",
    "print(f\"\\nPredictions summary:\")\n",
    "print(f\"  Predicted Benign (0): {(y_pred_test == 0).sum()}\")\n",
    "print(f\"  Predicted Malignant (1): {(y_pred_test == 1).sum()}\")\n",
    "print(f\"\\nActual distribution:\")\n",
    "print(f\"  Actual Benign (0): {(y_test == 0).sum()}\")\n",
    "print(f\"  Actual Malignant (1): {(y_test == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113818fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some sample predictions\n",
    "print(\"Sample predictions on test set (first 10):\")\n",
    "print(\"\\nActual | Predicted | Probability\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(10):\n",
    "    actual = y_test[i]\n",
    "    pred = y_pred_test[i]\n",
    "    prob = y_pred_prob_test[i]\n",
    "    match = \"✓\" if actual == pred else \"✗\"\n",
    "    print(f\"  {actual}    |     {pred}      |  {prob:.4f}  {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194e943",
   "metadata": {},
   "source": [
    "---\n",
    "# Part C4: Assessment Metrics (3 Marks)\n",
    "\n",
    "Determine Accuracy, Precision, and Recall from scratch\n",
    "\n",
    "**Clinical Perspective on Recall:**\n",
    "- Recall = True Positive / (True Positive + False Negative)\n",
    "- In oncology screening, overlooking true cases (False Negative) has grave consequences\n",
    "- High Recall prioritizes case identification\n",
    "- Better to have unconfirmed suspicious findings than to miss actual pathology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics manually\n",
    "    \n",
    "    Args:\n",
    "        y_true: actual labels\n",
    "        y_pred: predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        accuracy, precision, recall\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix components\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))  # True Positives\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))  # True Negatives\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))  # False Positives\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))  # False Negatives\n",
    "    \n",
    "    # Accuracy: correct predictions / all predictions\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    # Precision: correct positive predictions / all positive predictions\n",
    "    # Measures how many predicted malignant are actually malignant\n",
    "    if (TP + FP) == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = TP / (TP + FP)\n",
    "    \n",
    "    # Recall: correct positive predictions / all actual positives\n",
    "    # Measures how many actual malignant cases we detected\n",
    "    if (TP + FN) == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = TP / (TP + FN)\n",
    "    \n",
    "    # Also return confusion matrix\n",
    "    cm = {'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN}\n",
    "    \n",
    "    return accuracy, precision, recall, cm\n",
    "\n",
    "print(\"Metrics function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b782058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics on test set\n",
    "accuracy, precision, recall, cm = compute_metrics(y_test, y_pred_test)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TEST SET EVALUATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives (TP):   {cm['TP']}\")\n",
    "print(f\"  True Negatives (TN):   {cm['TN']}\")\n",
    "print(f\"  False Positives (FP):  {cm['FP']}\")\n",
    "print(f\"  False Negatives (FN):  {cm['FN']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5982b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check on training set\n",
    "y_pred_train, _ = make_predictions(X_train, w_final, b_final, threshold=0.5)\n",
    "train_acc, train_prec, train_rec, train_cm = compute_metrics(y_train, y_pred_train)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TRAINING SET EVALUATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nAccuracy:  {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"Precision: {train_prec:.4f} ({train_prec*100:.2f}%)\")\n",
    "print(f\"Recall:    {train_rec:.4f} ({train_rec*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c079831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training set\n",
    "train_cm_array = np.array([[train_cm['TN'], train_cm['FP']], \n",
    "                            [train_cm['FN'], train_cm['TP']]])\n",
    "\n",
    "axes[0].imshow(train_cm_array, cmap='Blues', aspect='auto')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels(['Predicted 0', 'Predicted 1'])\n",
    "axes[0].set_yticklabels(['Actual 0', 'Actual 1'])\n",
    "axes[0].set_title('Training Set Confusion Matrix')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0].text(j, i, str(train_cm_array[i, j]), \n",
    "                    ha='center', va='center', color='white', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Test set\n",
    "test_cm_array = np.array([[cm['TN'], cm['FP']], \n",
    "                           [cm['FN'], cm['TP']]])\n",
    "\n",
    "axes[1].imshow(test_cm_array, cmap='Greens', aspect='auto')\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_xticklabels(['Predicted 0', 'Predicted 1'])\n",
    "axes[1].set_yticklabels(['Actual 0', 'Actual 1'])\n",
    "axes[1].set_title('Test Set Confusion Matrix')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, str(test_cm_array[i, j]), \n",
    "                    ha='center', va='center', color='white', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrices plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics side by side\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall']\n",
    "train_scores = [train_acc, train_prec, train_rec]\n",
    "test_scores = [accuracy, precision, recall]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, train_scores, width, label='Training Set', color='skyblue', edgecolor='black')\n",
    "ax.bar(x + width/2, test_scores, width, label='Test Set', color='lightcoral', edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Metrics Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (train, test) in enumerate(zip(train_scores, test_scores)):\n",
    "    ax.text(i - width/2, train + 0.02, f'{train:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    ax.text(i + width/2, test + 0.02, f'{test:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Metrics comparison plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d776e2c",
   "metadata": {},
   "source": [
    "## Why Recall is Crucial in Cancer Detection\n",
    "\n",
    "In our test set:\n",
    "- **Recall = {:.4f} ({:.2f}%)**\n",
    "\n",
    "This means we correctly identified **{:.0f} out of {:.0f}** actual malignant cases.\n",
    "\n",
    "**Why this matters for healthcare:**\n",
    "\n",
    "1. **False Negatives are Dangerous**: Missing a cancer case (FN={}) could delay critical treatment\n",
    "2. **False Positives are Less Harmful**: Extra screening (FP={}) is inconvenient but safe\n",
    "3. **Better to Err on Safe Side**: In medical diagnosis, we'd rather have more false alarms than miss real cases\n",
    "4. **Recall vs Precision Trade-off**: High recall sometimes means lower precision, but that's acceptable here\n",
    "\n",
    "**Our Model's Performance:**\n",
    "- Caught **{:.1f}%** of actual cancer cases\n",
    "- This is a strong recall for medical application\n",
    "- Only {:.0f} cases were missed (False Negatives)\n",
    "\"\"\".format(recall, recall*100, cm['TP'], cm['TP']+cm['FN'], cm['FN'], cm['FP'],\n",
    "          recall*100, cm['FN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PART C SUMMARY: MODEL TRAINING & EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n✓ TRAIN-TEST SPLIT:\")\n",
    "print(f\"  Training samples: {X_train.shape[0]} (80%)\")\n",
    "print(f\"  Test samples: {X_test.shape[0]} (20%)\")\n",
    "\n",
    "print(\"\\n✓ MODEL TRAINING:\")\n",
    "print(f\"  Iterations: 3000\")\n",
    "print(f\"  Learning rate: 0.1\")\n",
    "print(f\"  Initial cost: {cost_history[0]:.6f}\")\n",
    "print(f\"  Final cost: {cost_history[-1]:.6f}\")\n",
    "print(f\"  Improvement: {(cost_history[0]-cost_history[-1])/cost_history[0]*100:.2f}%\")\n",
    "\n",
    "print(\"\\n✓ TEST SET PREDICTIONS:\")\n",
    "print(f\"  Total samples: {len(y_test)}\")\n",
    "print(f\"  Threshold: 0.5\")\n",
    "\n",
    "print(\"\\n✓ EVALUATION METRICS (TEST SET):\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"  Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n✓ CONFUSION MATRIX (TEST SET):\")\n",
    "print(f\"  TP (Correctly identified malignant): {cm['TP']}\")\n",
    "print(f\"  TN (Correctly identified benign): {cm['TN']}\")\n",
    "print(f\"  FP (False alarms): {cm['FP']}\")\n",
    "print(f\"  FN (Missed cancer cases): {cm['FN']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ready for Part D: Business & Healthcare Insights\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
